{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Answer_Extractor_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "80b8d53b7e554790832c3ddedc634131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_39538b2d5fe7432183f9aa37f7961d33",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d61c225e726f458bbf9e29dfefa553b2",
              "IPY_MODEL_7c8892ccc645404984675cebb07e1090"
            ]
          }
        },
        "39538b2d5fe7432183f9aa37f7961d33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d61c225e726f458bbf9e29dfefa553b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2a29f3054b34a7f8163eab1be9466b5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 512,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 512,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c8d3aa814fd4f34b69cfac7662a2e59"
          }
        },
        "7c8892ccc645404984675cebb07e1090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ed0f42cd0444b298621db6867ccda57",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 512/512 [01:03&lt;00:00, 8.01B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7154144a7c3841528c42c3af1893eb11"
          }
        },
        "d2a29f3054b34a7f8163eab1be9466b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c8d3aa814fd4f34b69cfac7662a2e59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ed0f42cd0444b298621db6867ccda57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7154144a7c3841528c42c3af1893eb11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32a920242fce4b6e8c400ba5ecd14959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b76420ee3572492da49f57834d6cba87",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0257e5a8b17143329ef5178d8a74f614",
              "IPY_MODEL_ea39e4507abb4ff3a12891f101959163"
            ]
          }
        },
        "b76420ee3572492da49f57834d6cba87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0257e5a8b17143329ef5178d8a74f614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4f6714ca20a3476d8eedba10eb4e3640",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 5069051,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5069051,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df0736c91bf04b159b1e3a08a0220c1f"
          }
        },
        "ea39e4507abb4ff3a12891f101959163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_41aa656e1ee94c40bae8af84be1566c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 5.07M/5.07M [00:02&lt;00:00, 2.32MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9627fc88be6b475d8cab04efbf0a2b38"
          }
        },
        "4f6714ca20a3476d8eedba10eb4e3640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df0736c91bf04b159b1e3a08a0220c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41aa656e1ee94c40bae8af84be1566c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9627fc88be6b475d8cab04efbf0a2b38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86b12e9ff8554b27a40425a9e30f2d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_84537af88e6c4bceac4f7c37e718acea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8fe6dfc9291f4a70a26a1948106a3f60",
              "IPY_MODEL_d99c2d11602d4534bcbfc064e5735201"
            ]
          }
        },
        "84537af88e6c4bceac4f7c37e718acea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8fe6dfc9291f4a70a26a1948106a3f60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81241631caf645d9b0ac6280e8e52ca6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1115590446,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1115590446,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a1a18f000584773b8b8e4f2d4177c08"
          }
        },
        "d99c2d11602d4534bcbfc064e5735201": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eba7a0fdb98c4a7d9564d0acb36b4af7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.12G/1.12G [01:01&lt;00:00, 18.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec90d0a63e9c4b5d92efc26681bfcd9c"
          }
        },
        "81241631caf645d9b0ac6280e8e52ca6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a1a18f000584773b8b8e4f2d4177c08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eba7a0fdb98c4a7d9564d0acb36b4af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec90d0a63e9c4b5d92efc26681bfcd9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mbwd9FLPH2v",
        "outputId": "a71996f6-ea39-41cd-c137-7715ccab6bc7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOGz8P9zwbPL",
        "outputId": "705d171b-2078-459f-eeec-1fb9d2915e6b"
      },
      "source": [
        "cd /content/gdrive/My Drive/Colab Notebooks/NLP/BERT_SQUAD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/NLP/BERT_SQUAD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq40m-8RPP58",
        "outputId": "64d68310-ed9a-4158-9699-09da0fa73e20"
      },
      "source": [
        "!pip install transformers==3.5.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==3.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 12.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 9.1MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 5.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 5.8MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 5.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 5.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 5.9MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 5.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 5.9MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 5.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 5.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 5.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 5.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 5.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (1.19.5)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 16.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (0.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (3.12.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.5.1) (2.23.0)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.5.1) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.5.1) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers==3.5.1) (53.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.5.1) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=ced1f8123d2711f24cadec59c3383c06f1236ede8c3b60b6744f478f0456f115\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iofrhIAYOzrc"
      },
      "source": [
        "import json, collections, os, random, glob, math, string, re, torch\n",
        "import numpy as np\n",
        "import timeit\n",
        "from tqdm import trange, tqdm_notebook as tqdm \n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from transformers import WEIGHTS_NAME, BertConfig, BertForQuestionAnswering, BertTokenizerFast, BasicTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from transformers.tokenization_bert import whitespace_tokenize\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "from transformers import RobertaConfig, RobertaForQuestionAnswering, RobertaTokenizer\n",
        "from transformers import XLMRobertaConfig, XLMRobertaForQuestionAnswering, XLMRobertaTokenizer\n",
        "from transformers import AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00--JDnqUMAj"
      },
      "source": [
        "import logging\n",
        "import math\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(s)))\n",
        "\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def get_raw_scores(examples, preds):\n",
        "    \"\"\"\n",
        "    Computes the exact and f1 scores from the examples and the model predictions\n",
        "    \"\"\"\n",
        "    exact_scores = {}\n",
        "    f1_scores = {}\n",
        "\n",
        "    for example in examples:\n",
        "        qas_id = example.qas_id\n",
        "        gold_answers = [answer[\"text\"] for answer in example.answers if normalize_answer(answer[\"text\"])]\n",
        "\n",
        "        if not gold_answers:\n",
        "            # For unanswerable questions, only correct answer is empty string\n",
        "            gold_answers = [\"\"]\n",
        "\n",
        "        if qas_id not in preds:\n",
        "            print(\"Missing prediction for %s\" % qas_id)\n",
        "            continue\n",
        "\n",
        "        prediction = preds[qas_id]\n",
        "        exact_scores[qas_id] = max(compute_exact(a, prediction) for a in gold_answers)\n",
        "        f1_scores[qas_id] = max(compute_f1(a, prediction) for a in gold_answers)\n",
        "\n",
        "    return exact_scores, f1_scores\n",
        "\n",
        "\n",
        "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
        "    new_scores = {}\n",
        "    for qid, s in scores.items():\n",
        "        pred_na = na_probs[qid] > na_prob_thresh\n",
        "        if pred_na:\n",
        "            new_scores[qid] = float(not qid_to_has_ans[qid])\n",
        "        else:\n",
        "            new_scores[qid] = s\n",
        "    return new_scores\n",
        "\n",
        "\n",
        "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
        "    if not qid_list:\n",
        "        total = len(exact_scores)\n",
        "        return collections.OrderedDict(\n",
        "            [\n",
        "                (\"exact\", 100.0 * sum(exact_scores.values()) / total),\n",
        "                (\"f1\", 100.0 * sum(f1_scores.values()) / total),\n",
        "                (\"total\", total),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        total = len(qid_list)\n",
        "        return collections.OrderedDict(\n",
        "            [\n",
        "                (\"exact\", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
        "                (\"f1\", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
        "                (\"total\", total),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "def merge_eval(main_eval, new_eval, prefix):\n",
        "    for k in new_eval:\n",
        "        main_eval[\"%s_%s\" % (prefix, k)] = new_eval[k]\n",
        "\n",
        "\n",
        "def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n",
        "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "    cur_score = num_no_ans\n",
        "    best_score = cur_score\n",
        "    best_thresh = 0.0\n",
        "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "    for i, qid in enumerate(qid_list):\n",
        "        if qid not in scores:\n",
        "            continue\n",
        "        if qid_to_has_ans[qid]:\n",
        "            diff = scores[qid]\n",
        "        else:\n",
        "            if preds[qid]:\n",
        "                diff = -1\n",
        "            else:\n",
        "                diff = 0\n",
        "        cur_score += diff\n",
        "        if cur_score > best_score:\n",
        "            best_score = cur_score\n",
        "            best_thresh = na_probs[qid]\n",
        "\n",
        "    has_ans_score, has_ans_cnt = 0, 0\n",
        "    for qid in qid_list:\n",
        "        if not qid_to_has_ans[qid]:\n",
        "            continue\n",
        "        has_ans_cnt += 1\n",
        "\n",
        "        if qid not in scores:\n",
        "            continue\n",
        "        has_ans_score += scores[qid]\n",
        "\n",
        "    return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt\n",
        "\n",
        "\n",
        "def find_all_best_thresh_v2(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "    best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "    best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "    main_eval[\"best_exact\"] = best_exact\n",
        "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
        "    main_eval[\"best_f1\"] = best_f1\n",
        "    main_eval[\"best_f1_thresh\"] = f1_thresh\n",
        "    main_eval[\"has_ans_exact\"] = has_ans_exact\n",
        "    main_eval[\"has_ans_f1\"] = has_ans_f1\n",
        "\n",
        "\n",
        "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
        "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "    cur_score = num_no_ans\n",
        "    best_score = cur_score\n",
        "    best_thresh = 0.0\n",
        "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "    for _, qid in enumerate(qid_list):\n",
        "        if qid not in scores:\n",
        "            continue\n",
        "        if qid_to_has_ans[qid]:\n",
        "            diff = scores[qid]\n",
        "        else:\n",
        "            if preds[qid]:\n",
        "                diff = -1\n",
        "            else:\n",
        "                diff = 0\n",
        "        cur_score += diff\n",
        "        if cur_score > best_score:\n",
        "            best_score = cur_score\n",
        "            best_thresh = na_probs[qid]\n",
        "    return 100.0 * best_score / len(scores), best_thresh\n",
        "\n",
        "\n",
        "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "    best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "\n",
        "    main_eval[\"best_exact\"] = best_exact\n",
        "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
        "    main_eval[\"best_f1\"] = best_f1\n",
        "    main_eval[\"best_f1_thresh\"] = f1_thresh\n",
        "\n",
        "\n",
        "def squad_evaluate(examples, preds, no_answer_probs=None, no_answer_probability_threshold=1.0):\n",
        "    qas_id_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n",
        "    has_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if has_answer]\n",
        "    no_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer]\n",
        "\n",
        "    if no_answer_probs is None:\n",
        "        no_answer_probs = {k: 0.0 for k in preds}\n",
        "\n",
        "    exact, f1 = get_raw_scores(examples, preds)\n",
        "\n",
        "    exact_threshold = apply_no_ans_threshold(\n",
        "        exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold\n",
        "    )\n",
        "    f1_threshold = apply_no_ans_threshold(f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)\n",
        "\n",
        "    evaluation = make_eval_dict(exact_threshold, f1_threshold)\n",
        "\n",
        "    if has_answer_qids:\n",
        "        has_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=has_answer_qids)\n",
        "        merge_eval(evaluation, has_ans_eval, \"HasAns\")\n",
        "\n",
        "    if no_answer_qids:\n",
        "        no_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=no_answer_qids)\n",
        "        merge_eval(evaluation, no_ans_eval, \"NoAns\")\n",
        "\n",
        "    if no_answer_probs:\n",
        "        find_all_best_thresh(evaluation, preds, exact, f1, no_answer_probs, qas_id_to_has_answer)\n",
        "\n",
        "    return evaluation\n",
        "\n",
        "\n",
        "def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False, tokenizer=None):\n",
        "    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
        "\n",
        "    # When we created the data, we kept track of the alignment between original\n",
        "    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
        "    # now `orig_text` contains the span of our original text corresponding to the\n",
        "    # span that we predicted.\n",
        "    #\n",
        "    # However, `orig_text` may contain extra characters that we don't want in\n",
        "    # our prediction.\n",
        "    #\n",
        "    # For example, let's say:\n",
        "    #   pred_text = steve smith\n",
        "    #   orig_text = Steve Smith's\n",
        "    #\n",
        "    # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
        "    #\n",
        "    # We don't want to return `pred_text` because it's already been normalized\n",
        "    # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
        "    # our tokenizer does additional normalization like stripping accent\n",
        "    # characters).\n",
        "    #\n",
        "    # What we really want to return is \"Steve Smith\".\n",
        "    #\n",
        "    # Therefore, we have to apply a semi-complicated alignment heuristic between\n",
        "    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n",
        "    # can fail in certain cases in which case we just return `orig_text`.\n",
        "\n",
        "    def _strip_spaces(text):\n",
        "        ns_chars = []\n",
        "        ns_to_s_map = collections.OrderedDict()\n",
        "        for (i, c) in enumerate(text):\n",
        "            if c == \" \":\n",
        "                continue\n",
        "            ns_to_s_map[len(ns_chars)] = i\n",
        "            ns_chars.append(c)\n",
        "        ns_text = \"\".join(ns_chars)\n",
        "        return (ns_text, ns_to_s_map)\n",
        "\n",
        "    # We first tokenize `orig_text`, strip whitespace from the result\n",
        "    # and `pred_text`, and check if they are the same length. If they are\n",
        "    # NOT the same length, the heuristic has failed. If they are the same\n",
        "    # length, we assume the characters are one-to-one aligned.\n",
        "    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "\n",
        "    tok_text = \" \".join(tokenizer.tokenize(orig_text)) if tokenizer else orig_text\n",
        "\n",
        "    start_position = tok_text.find(pred_text)\n",
        "    if start_position == -1:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
        "        return orig_text\n",
        "    end_position = start_position + len(pred_text) - 1\n",
        "\n",
        "    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
        "    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
        "\n",
        "    if len(orig_ns_text) != len(tok_ns_text):\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\", orig_ns_text, tok_ns_text)\n",
        "        return orig_text\n",
        "\n",
        "    # We then project the characters in `pred_text` back to `orig_text` using\n",
        "    # the character-to-character alignment.\n",
        "    tok_s_to_ns_map = {}\n",
        "    for (i, tok_index) in tok_ns_to_s_map.items():\n",
        "        tok_s_to_ns_map[tok_index] = i\n",
        "\n",
        "    orig_start_position = None\n",
        "    if start_position in tok_s_to_ns_map:\n",
        "        ns_start_position = tok_s_to_ns_map[start_position]\n",
        "        if ns_start_position in orig_ns_to_s_map:\n",
        "            orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
        "\n",
        "    if orig_start_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map start position\")\n",
        "        return orig_text\n",
        "\n",
        "    orig_end_position = None\n",
        "    if end_position in tok_s_to_ns_map:\n",
        "        ns_end_position = tok_s_to_ns_map[end_position]\n",
        "        if ns_end_position in orig_ns_to_s_map:\n",
        "            orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
        "\n",
        "    if orig_end_position is None:\n",
        "        if verbose_logging:\n",
        "            logger.info(\"Couldn't map end position\")\n",
        "        return orig_text\n",
        "\n",
        "    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n",
        "    return output_text\n",
        "\n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n",
        "\n",
        "\n",
        "def _compute_softmax(scores):\n",
        "    \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
        "    if not scores:\n",
        "        return []\n",
        "\n",
        "    max_score = None\n",
        "    for score in scores:\n",
        "        if max_score is None or score > max_score:\n",
        "            max_score = score\n",
        "\n",
        "    exp_scores = []\n",
        "    total_sum = 0.0\n",
        "    for score in scores:\n",
        "        x = math.exp(score - max_score)\n",
        "        exp_scores.append(x)\n",
        "        total_sum += x\n",
        "\n",
        "    probs = []\n",
        "    for score in exp_scores:\n",
        "        probs.append(score / total_sum)\n",
        "    return probs\n",
        "\n",
        "\n",
        "def compute_predictions_logits(\n",
        "    all_examples,\n",
        "    all_features,\n",
        "    all_results,\n",
        "    n_best_size,\n",
        "    max_answer_length,\n",
        "    do_lower_case,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    verbose_logging,\n",
        "    version_2_with_negative,\n",
        "    null_score_diff_threshold,\n",
        "    tokenizer, model_type\n",
        "):\n",
        "    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
        "    logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "    logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "    example_index_to_features = collections.defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
        "    )\n",
        "\n",
        "    all_predictions = collections.OrderedDict()\n",
        "    all_nbest_json = collections.OrderedDict()\n",
        "    scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "    for (example_index, example) in enumerate(all_examples):\n",
        "        features = example_index_to_features[example_index]\n",
        "\n",
        "        prelim_predictions = []\n",
        "        # keep track of the minimum score of null start+end of position 0\n",
        "        score_null = 1000000  # large and positive\n",
        "        min_null_feature_index = 0  # the paragraph slice with min null score\n",
        "        null_start_logit = 0  # the start logit at the slice with min null score\n",
        "        null_end_logit = 0  # the end logit at the slice with min null score\n",
        "        for (feature_index, feature) in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "            # if we could have irrelevant answers, get the min score of irrelevant\n",
        "            if version_2_with_negative:\n",
        "                feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
        "                if feature_null_score < score_null:\n",
        "                    score_null = feature_null_score\n",
        "                    min_null_feature_index = feature_index\n",
        "                    null_start_logit = result.start_logits[0]\n",
        "                    null_end_logit = result.end_logits[0]\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # We could hypothetically create invalid predictions, e.g., predict\n",
        "                    # that the start of the span is in the question. We throw out all\n",
        "                    # invalid predictions.\n",
        "                    if start_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if end_index >= len(feature.tokens):\n",
        "                        continue\n",
        "                    if start_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if end_index not in feature.token_to_orig_map:\n",
        "                        continue\n",
        "                    if not feature.token_is_max_context.get(start_index, False):\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "                    prelim_predictions.append(\n",
        "                        _PrelimPrediction(\n",
        "                            feature_index=feature_index,\n",
        "                            start_index=start_index,\n",
        "                            end_index=end_index,\n",
        "                            start_logit=result.start_logits[start_index],\n",
        "                            end_logit=result.end_logits[end_index],\n",
        "                        )\n",
        "                    )\n",
        "        if version_2_with_negative:\n",
        "            prelim_predictions.append(\n",
        "                _PrelimPrediction(\n",
        "                    feature_index=min_null_feature_index,\n",
        "                    start_index=0,\n",
        "                    end_index=0,\n",
        "                    start_logit=null_start_logit,\n",
        "                    end_logit=null_end_logit,\n",
        "                )\n",
        "            )\n",
        "        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n",
        "\n",
        "        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
        "        )\n",
        "\n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "            feature = features[pred.feature_index]\n",
        "            if pred.start_index > 0:  # this is a non-null prediction\n",
        "                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
        "                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
        "                if model_type == 'roberta':\n",
        "                    tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
        "                    tok_text = \" \".join(tok_text.strip().split())\n",
        "                    orig_text = \" \".join(orig_tokens)\n",
        "                    final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging, None)\n",
        "                else:\n",
        "                    tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "                    # De-tokenize WordPieces that have been split off.\n",
        "                    tok_text = tok_text.replace(\" ##\", \"\")\n",
        "                    tok_text = tok_text.replace(\"##\", \"\")\n",
        "                    # Clean whitespace\n",
        "                    tok_text = tok_text.strip()\n",
        "                    tok_text = \" \".join(tok_text.split())\n",
        "                    orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "                    final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging, tokenizer)\n",
        "                if final_text in seen_predictions:\n",
        "                    continue\n",
        "\n",
        "                seen_predictions[final_text] = True\n",
        "            else:\n",
        "                final_text = \"\"\n",
        "                seen_predictions[final_text] = True\n",
        "\n",
        "            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n",
        "        # if we didn't include the empty option in the n-best, include it\n",
        "        if version_2_with_negative:\n",
        "            if \"\" not in seen_predictions:\n",
        "                nbest.append(_NbestPrediction(text=\"\", start_logit=null_start_logit, end_logit=null_end_logit))\n",
        "\n",
        "            # In very rare edge cases we could only have single null prediction.\n",
        "            # So we just create a nonce prediction in this case to avoid failure.\n",
        "            if len(nbest) == 1:\n",
        "                nbest.insert(0, _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "        # In very rare edge cases we could have no valid predictions. So we\n",
        "        # just create a nonce prediction in this case to avoid failure.\n",
        "        if not nbest:\n",
        "            nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "        assert len(nbest) >= 1\n",
        "\n",
        "        total_scores = []\n",
        "        best_non_null_entry = None\n",
        "        for entry in nbest:\n",
        "            total_scores.append(entry.start_logit + entry.end_logit)\n",
        "            if not best_non_null_entry:\n",
        "                if entry.text:\n",
        "                    best_non_null_entry = entry\n",
        "\n",
        "        probs = _compute_softmax(total_scores)\n",
        "\n",
        "        nbest_json = []\n",
        "        for (i, entry) in enumerate(nbest):\n",
        "            output = collections.OrderedDict()\n",
        "            output[\"text\"] = entry.text\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"start_logit\"] = entry.start_logit\n",
        "            output[\"end_logit\"] = entry.end_logit\n",
        "            nbest_json.append(output)\n",
        "\n",
        "        assert len(nbest_json) >= 1\n",
        "\n",
        "        if not version_2_with_negative:\n",
        "            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
        "        else:\n",
        "            # predict \"\" iff the null score - the score of best non-null > threshold\n",
        "            score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)\n",
        "            scores_diff_json[example.qas_id] = score_diff\n",
        "            if score_diff > null_score_diff_threshold:\n",
        "                all_predictions[example.qas_id] = \"\"\n",
        "            else:\n",
        "                all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "        all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "    with open(output_prediction_file, \"w\") as writer:\n",
        "        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "\n",
        "    with open(output_nbest_file, \"w\") as writer:\n",
        "        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "\n",
        "    if version_2_with_negative:\n",
        "        with open(output_null_log_odds_file, \"w\") as writer:\n",
        "            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "\n",
        "def compute_predictions_log_probs(\n",
        "    all_examples,\n",
        "    all_features,\n",
        "    all_results,\n",
        "    n_best_size,\n",
        "    max_answer_length,\n",
        "    output_prediction_file,\n",
        "    output_nbest_file,\n",
        "    output_null_log_odds_file,\n",
        "    start_n_top,\n",
        "    end_n_top,\n",
        "    version_2_with_negative,\n",
        "    tokenizer,\n",
        "    verbose_logging,\n",
        "):\n",
        "    \"\"\" XLNet write prediction logic (more complex than Bert's).\n",
        "        Write final predictions to the json file and log-odds of null if needed.\n",
        "\n",
        "        Requires utils_squad_evaluate.py\n",
        "    \"\"\"\n",
        "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_log_prob\", \"end_log_prob\"]\n",
        "    )\n",
        "\n",
        "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"NbestPrediction\", [\"text\", \"start_log_prob\", \"end_log_prob\"]\n",
        "    )\n",
        "\n",
        "    logger.info(\"Writing predictions to: %s\", output_prediction_file)\n",
        "    # logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "    example_index_to_features = collections.defaultdict(list)\n",
        "    for feature in all_features:\n",
        "        example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "    unique_id_to_result = {}\n",
        "    for result in all_results:\n",
        "        unique_id_to_result[result.unique_id] = result\n",
        "\n",
        "    all_predictions = collections.OrderedDict()\n",
        "    all_nbest_json = collections.OrderedDict()\n",
        "    scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "    for (example_index, example) in enumerate(all_examples):\n",
        "        features = example_index_to_features[example_index]\n",
        "\n",
        "        prelim_predictions = []\n",
        "        # keep track of the minimum score of null start+end of position 0\n",
        "        score_null = 1000000  # large and positive\n",
        "\n",
        "        for (feature_index, feature) in enumerate(features):\n",
        "            result = unique_id_to_result[feature.unique_id]\n",
        "\n",
        "            cur_null_score = result.cls_logits\n",
        "\n",
        "            # if we could have irrelevant answers, get the min score of irrelevant\n",
        "            score_null = min(score_null, cur_null_score)\n",
        "\n",
        "            for i in range(start_n_top):\n",
        "                for j in range(end_n_top):\n",
        "                    start_log_prob = result.start_logits[i]\n",
        "                    start_index = result.start_top_index[i]\n",
        "\n",
        "                    j_index = i * end_n_top + j\n",
        "\n",
        "                    end_log_prob = result.end_logits[j_index]\n",
        "                    end_index = result.end_top_index[j_index]\n",
        "\n",
        "                    # We could hypothetically create invalid predictions, e.g., predict\n",
        "                    # that the start of the span is in the question. We throw out all\n",
        "                    # invalid predictions.\n",
        "                    if start_index >= feature.paragraph_len - 1:\n",
        "                        continue\n",
        "                    if end_index >= feature.paragraph_len - 1:\n",
        "                        continue\n",
        "\n",
        "                    if not feature.token_is_max_context.get(start_index, False):\n",
        "                        continue\n",
        "                    if end_index < start_index:\n",
        "                        continue\n",
        "                    length = end_index - start_index + 1\n",
        "                    if length > max_answer_length:\n",
        "                        continue\n",
        "\n",
        "                    prelim_predictions.append(\n",
        "                        _PrelimPrediction(\n",
        "                            feature_index=feature_index,\n",
        "                            start_index=start_index,\n",
        "                            end_index=end_index,\n",
        "                            start_log_prob=start_log_prob,\n",
        "                            end_log_prob=end_log_prob,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "        prelim_predictions = sorted(\n",
        "            prelim_predictions, key=lambda x: (x.start_log_prob + x.end_log_prob), reverse=True\n",
        "        )\n",
        "\n",
        "        seen_predictions = {}\n",
        "        nbest = []\n",
        "        for pred in prelim_predictions:\n",
        "            if len(nbest) >= n_best_size:\n",
        "                break\n",
        "            feature = features[pred.feature_index]\n",
        "\n",
        "            # XLNet un-tokenizer\n",
        "            # Let's keep it simple for now and see if we need all this later.\n",
        "            #\n",
        "            # tok_start_to_orig_index = feature.tok_start_to_orig_index\n",
        "            # tok_end_to_orig_index = feature.tok_end_to_orig_index\n",
        "            # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n",
        "            # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n",
        "            # paragraph_text = example.paragraph_text\n",
        "            # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n",
        "\n",
        "            # Previously used Bert untokenizer\n",
        "            tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n",
        "            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "            orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n",
        "            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n",
        "\n",
        "            # Clean whitespace\n",
        "            tok_text = tok_text.strip()\n",
        "            tok_text = \" \".join(tok_text.split())\n",
        "            orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "            if hasattr(tokenizer, \"do_lower_case\"):\n",
        "                do_lower_case = tokenizer.do_lower_case\n",
        "            else:\n",
        "                do_lower_case = tokenizer.do_lowercase_and_remove_accent\n",
        "\n",
        "            final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n",
        "\n",
        "            if final_text in seen_predictions:\n",
        "                continue\n",
        "\n",
        "            seen_predictions[final_text] = True\n",
        "\n",
        "            nbest.append(\n",
        "                _NbestPrediction(text=final_text, start_log_prob=pred.start_log_prob, end_log_prob=pred.end_log_prob)\n",
        "            )\n",
        "\n",
        "        # In very rare edge cases we could have no valid predictions. So we\n",
        "        # just create a nonce prediction in this case to avoid failure.\n",
        "        if not nbest:\n",
        "            nbest.append(_NbestPrediction(text=\"\", start_log_prob=-1e6, end_log_prob=-1e6))\n",
        "\n",
        "        total_scores = []\n",
        "        best_non_null_entry = None\n",
        "        for entry in nbest:\n",
        "            total_scores.append(entry.start_log_prob + entry.end_log_prob)\n",
        "            if not best_non_null_entry:\n",
        "                best_non_null_entry = entry\n",
        "\n",
        "        probs = _compute_softmax(total_scores)\n",
        "\n",
        "        nbest_json = []\n",
        "        for (i, entry) in enumerate(nbest):\n",
        "            output = collections.OrderedDict()\n",
        "            output[\"text\"] = entry.text\n",
        "            output[\"probability\"] = probs[i]\n",
        "            output[\"start_log_prob\"] = entry.start_log_prob\n",
        "            output[\"end_log_prob\"] = entry.end_log_prob\n",
        "            nbest_json.append(output)\n",
        "\n",
        "        assert len(nbest_json) >= 1\n",
        "        assert best_non_null_entry is not None\n",
        "\n",
        "        score_diff = score_null\n",
        "        scores_diff_json[example.qas_id] = score_diff\n",
        "        # note(zhiliny): always predict best_non_null_entry\n",
        "        # and the evaluation script will search for the best threshold\n",
        "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
        "\n",
        "        all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "    with open(output_prediction_file, \"w\") as writer:\n",
        "        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "\n",
        "    with open(output_nbest_file, \"w\") as writer:\n",
        "        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "\n",
        "    if version_2_with_negative:\n",
        "        with open(output_null_log_odds_file, \"w\") as writer:\n",
        "            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
        "\n",
        "    return all_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrrqwG2YsScx"
      },
      "source": [
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKNSUiYmOzrg"
      },
      "source": [
        "class SquadExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for the Squad dataset.\n",
        "    For examples without an answer, the start and end position are -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 qas_id,\n",
        "                 question_text,\n",
        "                 doc_tokens,\n",
        "                 orig_answer_text=None,\n",
        "                 start_position=None,\n",
        "                 end_position=None,\n",
        "                 is_impossible=None,\n",
        "                 answers=None):\n",
        "        self.qas_id = qas_id\n",
        "        self.question_text = question_text\n",
        "        self.doc_tokens = doc_tokens\n",
        "        self.orig_answer_text = orig_answer_text\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.is_impossible = is_impossible\n",
        "        self.answers = answers\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = \"\"\n",
        "        s += \"qas_id: %s\" % (self.qas_id)\n",
        "        s += \", question_text: %s\" % (\n",
        "            self.question_text)\n",
        "        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
        "        if self.start_position:\n",
        "            s += \", start_position: %d\" % (self.start_position)\n",
        "        if self.start_position:\n",
        "            s += \", end_position: %d\" % (self.end_position)\n",
        "        if self.start_position:\n",
        "            s += \", is_impossible: %r\" % (self.is_impossible)\n",
        "        if self.start_position:\n",
        "            s += \", answers: %r\" % (self.answers)\n",
        "        return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUYRVXoQO_KO"
      },
      "source": [
        "#res = []\n",
        "#for word, v in lst:\n",
        "    #if v == prev:\n",
        "    #    skip += 1\n",
        "    #else:\n",
        "    #    rank += skip + 1\n",
        "    #    skip = 0\n",
        "\n",
        "    #res.append( (word, v, rank) )    \n",
        "    #prev = v\n",
        "\n",
        "\n",
        "#Ranking example theo chiều  dài\n",
        "def rank_examples_by_question_length(examples):\n",
        "    \n",
        "    for item in examples:\n",
        "        print(item)\n",
        "\n",
        "\n",
        "#def rank_examples_by_answer_length(examples):\n",
        "\n",
        "#def rank_examples_by_passage_length(examples):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jJlBldNOzrj"
      },
      "source": [
        "def read_squad_examples(input_file, is_training):\n",
        "    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
        "\n",
        "    with open(input_file, \"r\", encoding='utf-8') as reader:\n",
        "        source = json.load(reader)\n",
        "        input_data = source[\"data\"]\n",
        "        version = source[\"version\"]\n",
        "\n",
        "    def is_whitespace(c):\n",
        "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    examples = []\n",
        "    for entry in input_data:\n",
        "        for paragraph in entry[\"paragraphs\"]:\n",
        "            paragraph_text = paragraph[\"context\"]\n",
        "            doc_tokens = []\n",
        "            char_to_word_offset = []\n",
        "            prev_is_whitespace = True\n",
        "            for c in paragraph_text:\n",
        "                if is_whitespace(c):\n",
        "                    prev_is_whitespace = True\n",
        "                else:\n",
        "                    if prev_is_whitespace:\n",
        "                        doc_tokens.append(c)\n",
        "                    else:\n",
        "                        doc_tokens[-1] += c\n",
        "                    prev_is_whitespace = False\n",
        "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
        "\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                qas_id = qa[\"id\"]\n",
        "                question_text = qa[\"question\"]\n",
        "                start_position = None\n",
        "                end_position = None\n",
        "                orig_answer_text = None\n",
        "                is_impossible = False\n",
        "                answers = []\n",
        "                if is_training:\n",
        "                    if version == \"v2.0\":\n",
        "                        is_impossible = qa[\"is_impossible\"]\n",
        "                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
        "                        #print(entry[\"title\"], qas_id)\n",
        "                        raise ValueError(\n",
        "                            \"For training, each question should have exactly 1 answer.\")\n",
        "                    if not is_impossible:\n",
        "                        answer = qa[\"answers\"][0]\n",
        "                        orig_answer_text = answer[\"text\"]\n",
        "                        answer_offset = answer[\"answer_start\"]\n",
        "                        answer_length = len(orig_answer_text)\n",
        "                        start_position = char_to_word_offset[answer_offset]\n",
        "                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n",
        "                        # Only add answers where the text can be exactly recovered from the\n",
        "                        # document. If this CAN'T happen it's likely due to weird Unicode\n",
        "                        # stuff so we will just skip the example.\n",
        "                        #\n",
        "                        # Note that this means for training mode, every example is NOT\n",
        "                        # guaranteed to be preserved.\n",
        "                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n",
        "                        cleaned_answer_text = \" \".join(whitespace_tokenize(orig_answer_text))\n",
        "                        if actual_text.find(cleaned_answer_text) == -1:\n",
        "                            print(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                            actual_text, cleaned_answer_text)\n",
        "                            continue\n",
        "                    else: \n",
        "                        start_position = -1\n",
        "                        end_position = -1\n",
        "                        orig_answer_text = \"\"\n",
        "                else:\n",
        "                    answers = qa[\"answers\"]\n",
        "\n",
        "                example = SquadExample(\n",
        "                    qas_id=qas_id,\n",
        "                    question_text=question_text,\n",
        "                    doc_tokens=doc_tokens,\n",
        "                    orig_answer_text=orig_answer_text,\n",
        "                    start_position=start_position,\n",
        "                    end_position=end_position,\n",
        "                    is_impossible=is_impossible,\n",
        "                    answers=answers)\n",
        "                \n",
        "                print(example)\n",
        "                type(example)\n",
        "                #qas_id: uit_01__08947_13_1, \n",
        "                #question_text: Dân tộc nào có nhiều nét tương đồng nhất với dân tộc Ê Đê?, \n",
        "                #doc_tokens: [Người Ê đê và người Gia Rai vốn cùng nguồn gốc từ một tộc người Orang Đê cổ được ghi chép khá nhiều trong các bia ký Champa, Khmer,...Orang Đê có thể là nhóm mà người Ê đê và Gia Rai gọi là Mdhur, trong văn hóa Mdhur có chứa đựng nhiều yếu tố văn hoá trung gian giữa người Ê đê và Gia Rai. Trong văn hóa Mdhur trước kia còn tồn tại tục hỏa táng người chết và bỏ tro trong chum, ché sau đó mới mang chôn cất trong nhà mồ, đây có thể là ảnh hưởng của đạo Hin-đu từ người Chăm. Xét về phương diện người Mdhur là cội nguồn xuất phát của người Ê đê và Gia Rai hiện đại. Trong lịch sử Orang Đê đã từng tồn tại các tiểu quốc sơ khai, với sự cai trị của các Mtao, Pơ Tao có thế lực trên một khu vực rộng lớn ở vùng người Gia Rai và Ê đê. Sự hình thành các tiểu quốc nhỏ là đặc điểm thường thấy ở các tộc người Đông Nam á:], \n",
        "                #start_position: 5, \n",
        "                #end_position: 6, \n",
        "                #is_impossible: False, \n",
        "                #answers: []\n",
        "\n",
        "                examples.append(example)\n",
        "                \n",
        "\n",
        "                #Ranking examples\n",
        "    return examples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8cQTmv1Ozrl"
      },
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 unique_id, \n",
        "                 example_index,\n",
        "                 doc_span_index,\n",
        "                 tokens,\n",
        "                 token_to_orig_map,\n",
        "                 token_is_max_context,\n",
        "                 input_ids,\n",
        "                 input_mask,\n",
        "                 segment_ids,\n",
        "                 cls_index,\n",
        "                 p_mask,\n",
        "                 paragraph_len,\n",
        "                 start_position=None,\n",
        "                 end_position=None,\n",
        "                 is_impossible=None):\n",
        "        \n",
        "        self.unique_id = unique_id\n",
        "        self.example_index = example_index\n",
        "        self.doc_span_index = doc_span_index\n",
        "        self.tokens = tokens\n",
        "        self.token_to_orig_map = token_to_orig_map\n",
        "        self.token_is_max_context = token_is_max_context\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.cls_index = cls_index\n",
        "        self.p_mask = p_mask\n",
        "        self.paragraph_len = paragraph_len\n",
        "        self.start_position = start_position\n",
        "        self.end_position = end_position\n",
        "        self.is_impossible = is_impossible"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycSIgb6sOzrp"
      },
      "source": [
        "def _improve_answer_span(doc_tokens, \n",
        "                         input_start, \n",
        "                         input_end, \n",
        "                         tokenizer,\n",
        "                         orig_answer_text, add_prefix_space=False):\n",
        "    \"\"\"\n",
        "    Returns tokenized answer spans that better match the annotated answer.\n",
        "    \"\"\"\n",
        "    \n",
        "    # print(doc_tokens)\n",
        "    # ['beyonce', 'gi', '##selle', 'knowles', '-', 'carter', '(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/', 'bee', '-', 'yo', '##n', '-', 'say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl', '-', 'group', 'destiny', \"'\", 's', 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'\", 's', 'best', '-', 'selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyonce', \"'\", 's', 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number', '-', 'one', 'singles', '\"', 'crazy', 'in', 'love', '\"', 'and', '\"', 'baby', 'boy', '\"', '.']\n",
        "\n",
        "    # print(input_start)\n",
        "    # 66\n",
        "    \n",
        "    # print(input_end) \n",
        "    # 69\n",
        "    \n",
        "    # print(tokenizer)\n",
        "    # <transformers.tokenization_bert.BertTokenizer object at 0x000001C1CE9D4F98>\n",
        "    \n",
        "    # print(orig_answer_text)\n",
        "    # in the late 1990s\n",
        "    \n",
        "    if add_prefix_space:\n",
        "        tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
        "    else:\n",
        "        tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))    # print(tok_answer_text)\n",
        "    # in the late 1990s\n",
        "\n",
        "    for new_start in range(input_start, input_end + 1):\n",
        "        for new_end in range(input_end, new_start - 1, -1):\n",
        "            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
        "            # print(text_span)\n",
        "            # in the late 1990s\n",
        "            if text_span == tok_answer_text:\n",
        "                return (new_start, new_end)\n",
        "\n",
        "    return (input_start, input_end)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_hTGOdQOzrt"
      },
      "source": [
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    \"\"\"\n",
        "    Check if this is the \"max context\" doc span for the token.\n",
        "    \"\"\"\n",
        "    \n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    \n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T92CZIpZOzrw"
      },
      "source": [
        "def convert_examples_to_features(examples, \n",
        "                                 tokenizer, \n",
        "                                 max_seq_length,\n",
        "                                 doc_stride,\n",
        "                                 max_query_length,\n",
        "                                 is_training,\n",
        "                                 cls_token_at_end=False,\n",
        "                                 cls_token=\"[CLS]\", \n",
        "                                 sep_token=\"[SEP]\", \n",
        "                                 pad_token=0,\n",
        "                                 add_prefix_space=False,\n",
        "                                 sequence_a_segment_id=0, \n",
        "                                 sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=0, \n",
        "                                 pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\"\n",
        "    Loads a data file into a list of `InputBatch`s.\n",
        "    \"\"\"\n",
        "    unique_id = 1000000000\n",
        "    \n",
        "    features = []\n",
        "    for (example_index, example) in enumerate(examples):\n",
        "        # print(example_index)\n",
        "        # 0\n",
        "        \n",
        "        # print(example)\n",
        "        # qas_id: 56be85543aeaaa14008c9063, \n",
        "        # question_text: When did Beyonce start becoming popular?,\n",
        "        # doc_tokens: [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".], \n",
        "        # start_position: 39, \n",
        "        # end_position: 42\n",
        "        \n",
        "        if add_prefix_space:\n",
        "            query_tokens = tokenizer.tokenize(example.question_text)\n",
        "        else:\n",
        "            query_tokens = tokenizer.tokenize(example.question_text)        # print(query_tokens)\n",
        "        # ['when', 'did', 'beyonce', 'start', 'becoming', 'popular', '?']\n",
        "    \n",
        "        if len(query_tokens) > max_query_length:\n",
        "            query_tokens = query_tokens[0:max_query_length]\n",
        "            \n",
        "        tok_to_orig_index = []\n",
        "        orig_to_tok_index = []\n",
        "        all_doc_tokens = []\n",
        "        \n",
        "        # `token`s are separated by whitespace; `sub_token`s are separated in a `token` by symbol\n",
        "        for (i, token) in enumerate(example.doc_tokens):\n",
        "            # print(i)\n",
        "            # 0\n",
        "            \n",
        "            # print(token)\n",
        "            # Beyoncé\n",
        "            \n",
        "            orig_to_tok_index.append(len(all_doc_tokens))\n",
        "            if add_prefix_space:\n",
        "                sub_tokens = tokenizer.tokenize(token)\n",
        "            else:\n",
        "                sub_tokens = tokenizer.tokenize(token)\n",
        "            # print(sub_tokens)\n",
        "            # ['beyonce']\n",
        "            # ['gi', '##selle']\n",
        "            # ['knowles', '-', 'carter']\n",
        "            # ['(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/']\n",
        "            # ...\n",
        "            for sub_token in sub_tokens:\n",
        "                tok_to_orig_index.append(i)\n",
        "                all_doc_tokens.append(sub_token)\n",
        "        \n",
        "        # print(tok_to_orig_index)\n",
        "        # [0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 6, 7, 7, 8, 8, 9, 10, 11, 12, 12, 13, 13, 14, 15, 16, 17, 17, 18, 19, 20, 21, 22, 22, 23, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 47, 47, 48, 48, 48, 49, 49, 49, 50, 50, 51, 52, 53, 54, 54, 55, 56, 56, 57, 58, 59, 60, 61, 62, 63, 63, 63, 64, 64, 64, 65, 66, 67, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 76, 76, 77, 78, 78, 79, 80, 81, 82, 82, 82, 82, 83, 84, 85, 86, 87, 88, 89, 90, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 101, 101, 102, 103, 103, 104, 105, 105, 106, 107, 107, 108, 108, 108]\n",
        "        \n",
        "        # print(orig_to_tok_index)\n",
        "        # [0, 1, 3, 6, 16, 23, 25, 26, 28, 30, 31, 32, 33, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 80, 83, 85, 86, 87, 88, 90, 91, 93, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 120, 121, 123, 124, 125, 126, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 153, 155, 156, 158, 159, 161]\n",
        "        \n",
        "        # print(all_doc_tokens)\n",
        "        # ['beyonce', 'gi', '##selle', 'knowles', '-', 'carter', '(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/', 'bee', '-', 'yo', '##n', '-', 'say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl', '-', 'group', 'destiny', \"'\", 's', 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'\", 's', 'best', '-', 'selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyonce', \"'\", 's', 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number', '-', 'one', 'singles', '\"', 'crazy', 'in', 'love', '\"', 'and', '\"', 'baby', 'boy', '\"', '.']\n",
        "        \n",
        "        tok_start_position = None\n",
        "        tok_end_position = None\n",
        "        \n",
        "        if is_training and example.is_impossible:\n",
        "            tok_start_position = -1\n",
        "            tok_end_position = -1\n",
        "        if is_training and not example.is_impossible:\n",
        "            tok_start_position = orig_to_tok_index[example.start_position]\n",
        "            if example.end_position < len(example.doc_tokens) - 1:\n",
        "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
        "            else:\n",
        "                tok_end_position = len(all_doc_tokens) - 1\n",
        "            # print(tok_start_position)\n",
        "            # 66\n",
        "            \n",
        "            # print(tok_end_position)\n",
        "            # 69\n",
        "            (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens, \n",
        "                                                                          tok_start_position, \n",
        "                                                                          tok_end_position, \n",
        "                                                                          tokenizer, \n",
        "                                                                          example.orig_answer_text)\n",
        "            # print(tok_start_position)\n",
        "            # 66\n",
        "            \n",
        "            # print(tok_end_position)\n",
        "            # 69\n",
        "            \n",
        "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "        \n",
        "        # We can have documents that are longer than the maximum sequence length. To deal with this we do a \n",
        "        # sliding window approach, where we take chunks of the up to our max length with a stride of `doc_stride`.\n",
        "        _DocSpan = collections.namedtuple(\"DocSpan\", [\"start\", \"length\"])\n",
        "        doc_spans = []\n",
        "        start_offset = 0\n",
        "        \n",
        "        while start_offset < len(all_doc_tokens):\n",
        "            # print(len(all_doc_tokens))\n",
        "            # 426\n",
        "            length = len(all_doc_tokens) - start_offset\n",
        "            if length > max_tokens_for_doc:\n",
        "                length = max_tokens_for_doc\n",
        "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "            # Take an example with stride\n",
        "            # \n",
        "            # print(doc_spans)\n",
        "            # [DocSpan(start=0, length=373)]\n",
        "            # \n",
        "            # In this case, `start` will move a `doc_strike`, 128, so the new `start` is 128  \n",
        "            # And the new `length` is 426 - 128 = 298\n",
        "            # \n",
        "            # [DocSpan(start=0, length=373), DocSpan(start=128, length=298)]\n",
        "            if start_offset + length == len(all_doc_tokens):\n",
        "                break\n",
        "            start_offset += min(length, doc_stride)\n",
        "        \n",
        "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "            \n",
        "            tokens = []\n",
        "            token_to_orig_map = {}\n",
        "            token_is_max_context = {}\n",
        "            segment_ids = []\n",
        "            # `p_mask`: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
        "            # Original TF implem also keeps the classification token (set to 0) (not sure why...)\n",
        "            p_mask = []\n",
        "\n",
        "            # `[CLS]` token at the beginning\n",
        "            if not cls_token_at_end:\n",
        "                tokens.append(cls_token)\n",
        "                segment_ids.append(cls_token_segment_id)\n",
        "                p_mask.append(0)\n",
        "                cls_index = 0\n",
        "                \n",
        "            # Query\n",
        "            for token in query_tokens:\n",
        "                tokens.append(token)\n",
        "                segment_ids.append(sequence_a_segment_id)\n",
        "                p_mask.append(1)\n",
        "\n",
        "            # [SEP] token\n",
        "            tokens.append(sep_token)\n",
        "            segment_ids.append(sequence_a_segment_id)\n",
        "            p_mask.append(1)\n",
        "\n",
        "            # Paragraph built based on `doc_span`\n",
        "            for i in range(doc_span.length):\n",
        "                split_token_index = doc_span.start + i\n",
        "                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "                is_max_context = _check_is_max_context(doc_spans, doc_span_index, split_token_index)\n",
        "                token_is_max_context[len(tokens)] = is_max_context\n",
        "                tokens.append(all_doc_tokens[split_token_index])\n",
        "                segment_ids.append(sequence_b_segment_id)\n",
        "                p_mask.append(0)\n",
        "            paragraph_len = doc_span.length\n",
        "\n",
        "            # [SEP] token\n",
        "            tokens.append(sep_token)\n",
        "            segment_ids.append(sequence_b_segment_id)\n",
        "            p_mask.append(1)\n",
        "\n",
        "            # [CLS] token at the end\n",
        "            if cls_token_at_end:\n",
        "                tokens.append(cls_token)\n",
        "                segment_ids.append(cls_token_segment_id)\n",
        "                p_mask.append(0)\n",
        "                # Index of classification token\n",
        "                cls_index = len(tokens) - 1\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            \n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            while len(input_ids) < max_seq_length:\n",
        "                input_ids.append(pad_token)\n",
        "                input_mask.append(0 if mask_padding_with_zero else 1)\n",
        "                segment_ids.append(pad_token_segment_id)\n",
        "                p_mask.append(1)\n",
        "\n",
        "            # print(input_ids)\n",
        "            # [101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773, 21025, 19358, 22815, 1011, 5708, 1006, 1013, 12170, 23432, 29715, 3501, 29678, 12325, 29685, 1013, 10506, 1011, 10930, 2078, 1011, 2360, 1007, 1006, 2141, 2244, 1018, 1010, 3261, 1007, 2003, 2019, 2137, 3220, 1010, 6009, 1010, 2501, 3135, 1998, 3883, 1012, 2141, 1998, 2992, 1999, 5395, 1010, 3146, 1010, 2016, 2864, 1999, 2536, 4823, 1998, 5613, 6479, 2004, 1037, 2775, 1010, 1998, 3123, 2000, 4476, 1999, 1996, 2397, 4134, 2004, 2599, 3220, 1997, 1054, 1004, 1038, 2611, 1011, 2177, 10461, 1005, 1055, 2775, 1012, 3266, 2011, 2014, 2269, 1010, 25436, 22815, 1010, 1996, 2177, 2150, 2028, 1997, 1996, 2088, 1005, 1055, 2190, 1011, 4855, 2611, 2967, 1997, 2035, 2051, 1012, 2037, 14221, 2387, 1996, 2713, 1997, 20773, 1005, 1055, 2834, 2201, 1010, 20754, 1999, 2293, 1006, 2494, 1007, 1010, 2029, 2511, 2014, 2004, 1037, 3948, 3063, 4969, 1010, 3687, 2274, 8922, 2982, 1998, 2956, 1996, 4908, 2980, 2531, 2193, 1011, 2028, 3895, 1000, 4689, 1999, 2293, 1000, 1998, 1000, 3336, 2879, 1000, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "            \n",
        "            # print(input_mask)\n",
        "            # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "            \n",
        "            # print(segment_ids)\n",
        "            # [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "            # Only `sequence_b_segment_id` is set to 1\n",
        "            \n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            span_is_impossible = example.is_impossible\n",
        "            start_position = None\n",
        "            end_position = None\n",
        "            \n",
        "            # Get `start_position` and `end_position`\n",
        "            if is_training and not span_is_impossible:\n",
        "                # For training, if our document chunk does not contain an annotation we throw it out, \n",
        "                # since there is nothing to predict.\n",
        "                doc_start = doc_span.start\n",
        "                doc_end = doc_span.start + doc_span.length - 1\n",
        "                out_of_span = False\n",
        "                if not (tok_start_position >= doc_start and\n",
        "                        tok_end_position <= doc_end):\n",
        "                    out_of_span = True\n",
        "                if out_of_span:\n",
        "                    start_position = 0\n",
        "                    end_position = 0\n",
        "                    span_is_impossible = True\n",
        "                else:\n",
        "                    doc_offset = len(query_tokens) + 2\n",
        "                    start_position = tok_start_position - doc_start + doc_offset\n",
        "                    end_position = tok_end_position - doc_start + doc_offset\n",
        "           \n",
        "            if is_training and span_is_impossible:\n",
        "                start_position = cls_index\n",
        "                end_position = cls_index\n",
        "        \n",
        "            # Display some examples\n",
        "            if example_index < 20:              \n",
        "                print(\"*** Example ***\")\n",
        "                # *** Example ***\n",
        "                print(\"unique_id: %s\" % (unique_id))\n",
        "                # unique_id: 1000000000\n",
        "                print(\"example_index: %s\" % (example_index))\n",
        "                # example_index: 0\n",
        "                print(\"doc_span_index: %s\" % (doc_span_index))\n",
        "                # doc_span_index: 0\n",
        "                print(\"tokens: %s\" % \" \".join(tokens))\n",
        "                # tokens: [CLS] when did beyonce start becoming popular ? [SEP] beyonce gi ##selle knowles - carter ( / bi ##ː ##ˈ ##j ##ɒ ##nse ##ɪ / bee - yo ##n - say ) ( born september 4 , 1981 ) is an american singer , songwriter , record producer and actress . born and raised in houston , texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of r & b girl - group destiny ' s child . managed by her father , mathew knowles , the group became one of the world ' s best - selling girl groups of all time . their hiatus saw the release of beyonce ' s debut album , dangerously in love ( 2003 ) , which established her as a solo artist worldwide , earned five grammy awards and featured the billboard hot 100 number - one singles \" crazy in love \" and \" baby boy \" . [SEP]\n",
        "                print(\"token_to_orig_map: %s\" % \" \".join([\n",
        "                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
        "                # token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:3 20:3 21:3 22:3 23:3 24:3 25:4 26:4 27:4 28:4 29:4 30:4 31:4 32:5 33:5 34:6 35:7 36:7 37:8 38:8 39:9 40:10 41:11 42:12 43:12 44:13 45:13 46:14 47:15 48:16 49:17 50:17 51:18 52:19 53:20 54:21 55:22 56:22 57:23 58:23 59:24 60:25 61:26 62:27 63:28 64:29 65:30 66:31 67:32 68:33 69:34 70:34 71:35 72:36 73:37 74:38 75:39 76:40 77:41 78:42 79:43 80:44 81:45 82:46 83:47 84:47 85:47 86:48 87:48 88:48 89:49 90:49 91:49 92:50 93:50 94:51 95:52 96:53 97:54 98:54 99:55 100:56 101:56 102:57 103:58 104:59 105:60 106:61 107:62 108:63 109:63 110:63 111:64 112:64 113:64 114:65 115:66 116:67 117:68 118:69 119:69 120:70 121:71 122:72 123:73 124:74 125:75 126:76 127:76 128:76 129:77 130:78 131:78 132:79 133:80 134:81 135:82 136:82 137:82 138:82 139:83 140:84 141:85 142:86 143:87 144:88 145:89 146:90 147:90 148:91 149:92 150:93 151:94 152:95 153:96 154:97 155:98 156:99 157:100 158:101 159:101 160:101 161:102 162:103 163:103 164:104 165:105 166:105 167:106 168:107 169:107 170:108 171:108 172:108\n",
        "                print(\"token_is_max_context: %s\" % \" \".join([\n",
        "                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
        "                ]))\n",
        "                # token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True\n",
        "                print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "                # input_ids: 101 2043 2106 20773 2707 3352 2759 1029 102 20773 21025 19358 22815 1011 5708 1006 1013 12170 23432 29715 3501 29678 12325 29685 1013 10506 1011 10930 2078 1011 2360 1007 1006 2141 2244 1018 1010 3261 1007 2003 2019 2137 3220 1010 6009 1010 2501 3135 1998 3883 1012 2141 1998 2992 1999 5395 1010 3146 1010 2016 2864 1999 2536 4823 1998 5613 6479 2004 1037 2775 1010 1998 3123 2000 4476 1999 1996 2397 4134 2004 2599 3220 1997 1054 1004 1038 2611 1011 2177 10461 1005 1055 2775 1012 3266 2011 2014 2269 1010 25436 22815 1010 1996 2177 2150 2028 1997 1996 2088 1005 1055 2190 1011 4855 2611 2967 1997 2035 2051 1012 2037 14221 2387 1996 2713 1997 20773 1005 1055 2834 2201 1010 20754 1999 2293 1006 2494 1007 1010 2029 2511 2014 2004 1037 3948 3063 4969 1010 3687 2274 8922 2982 1998 2956 1996 4908 2980 2531 2193 1011 2028 3895 1000 4689 1999 2293 1000 1998 1000 3336 2879 1000 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "                print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "                # input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "                print(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "                # segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
        "                if is_training and span_is_impossible:\n",
        "                    print(\"impossible example\")\n",
        "                if is_training and not span_is_impossible:\n",
        "                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
        "                    print(\"start_position: %d\" % (start_position))\n",
        "                    # start_position: 75\n",
        "                    print(\"end_position: %d\" % (end_position))\n",
        "                    # end_position: 78\n",
        "                    print(\"answer: %s\" % (answer_text))\n",
        "                    # answer: in the late 1990s\n",
        "\n",
        "            features.append(\n",
        "                InputFeatures(\n",
        "                    unique_id=unique_id,\n",
        "                    example_index=example_index,\n",
        "                    doc_span_index=doc_span_index,\n",
        "                    tokens=tokens,\n",
        "                    token_to_orig_map=token_to_orig_map,\n",
        "                    token_is_max_context=token_is_max_context,\n",
        "                    input_ids=input_ids,\n",
        "                    input_mask=input_mask,\n",
        "                    segment_ids=segment_ids,\n",
        "                    cls_index=cls_index,\n",
        "                    p_mask=p_mask,\n",
        "                    paragraph_len=paragraph_len,\n",
        "                    start_position=start_position,\n",
        "                    end_position=end_position,\n",
        "                    is_impossible=span_is_impossible)\n",
        "            )\n",
        "            \n",
        "            unique_id += 1\n",
        "\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQSJ7pf7Ozrz"
      },
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False, model_type='bert'):\n",
        "\n",
        "    \n",
        "    # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "    if args[\"local_rank\"] not in [-1, 0] and not evaluate:\n",
        "        torch.distributed.barrier()  \n",
        "    \n",
        "    # Load data features from cache or dataset file\n",
        "    input_file = args[\"predict_file\"] if evaluate else args[\"train_file\"]\n",
        "    cached_features_file = os.path.join(\n",
        "        os.path.dirname(input_file), \n",
        "        'cached_{}_{}_{}'.format(\n",
        "            'dev' if evaluate else 'train',\n",
        "            list(filter(None, args[\"model_name_or_path\"].split('/'))).pop(),\n",
        "            str(args[\"max_seq_length\"])\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    if os.path.exists(cached_features_file) and not args[\"overwrite_cache\"] and not output_examples:\n",
        "        print(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        print(\"Creating features from dataset file at %s\", input_file)\n",
        "        \n",
        "        # Call `read_squad_examples()`\n",
        "        examples = read_squad_examples(input_file=input_file,\n",
        "                                       is_training=not evaluate)\n",
        "\n",
        "        # Call `convert_examples_to_features()`\n",
        "        features = convert_examples_to_features(examples, \n",
        "                                                tokenizer, \n",
        "                                                max_seq_length=args[\"max_seq_length\"],\n",
        "                                                doc_stride=args[\"doc_stride\"],\n",
        "                                                max_query_length=args[\"max_query_length\"],\n",
        "                                                is_training=not evaluate,\n",
        "                                                cls_token_at_end=False,\n",
        "                                                cls_token = '<s>' if model_type == 'roberta' else '[CLS]',\n",
        "                                                sep_token = '</s>' if model_type == 'roberta' else '[SEP]',\n",
        "                                                pad_token = 1 if model_type == 'roberta' else  0,\n",
        "                                                add_prefix_space = True if model_type == 'roberta' else  False,\n",
        "                                                sequence_a_segment_id=0, # 'pad_token': '[PAD]'\n",
        "                                                sequence_b_segment_id=1,\n",
        "                                                cls_token_segment_id=0, \n",
        "                                                pad_token_segment_id=0,\n",
        "                                                mask_padding_with_zero=True)\n",
        "        \n",
        "        if args[\"local_rank\"] in [-1, 0]:\n",
        "            print(\"Saving features into cached file %s\", cached_features_file)\n",
        "            torch.save(features, cached_features_file)\n",
        "\n",
        "    # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "    if args[\"local_rank\"] == 0 and not evaluate:\n",
        "        torch.distributed.barrier()  \n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
        "    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
        "    \n",
        "    if evaluate:\n",
        "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, \n",
        "                                all_input_mask, \n",
        "                                all_segment_ids,\n",
        "                                all_example_index, \n",
        "                                all_cls_index, \n",
        "                                all_p_mask)\n",
        "    else:\n",
        "        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n",
        "        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, \n",
        "                                all_input_mask, \n",
        "                                all_segment_ids,\n",
        "                                all_start_positions, \n",
        "                                all_end_positions,\n",
        "                                all_cls_index, \n",
        "                                all_p_mask)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUUo4l-lOzr3"
      },
      "source": [
        "def set_seed(args):\n",
        "    random.seed(args[\"seed\"])\n",
        "    np.random.seed(args[\"seed\"])\n",
        "    torch.manual_seed(args[\"seed\"])\n",
        "    if args[\"n_gpu\"] > 0:\n",
        "        torch.cuda.manual_seed_all(args[\"seed\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjOgJoftOzr6"
      },
      "source": [
        "def train(args,        \n",
        "          train_dataset,\n",
        "          model,\n",
        "          tokenizer):\n",
        "    \"\"\"\n",
        "    Train the model.\n",
        "    \"\"\"\n",
        "\n",
        "    #DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None)\n",
        "    \n",
        "    args[\"train_batch_size\"] = args[\"per_gpu_train_batch_size\"] * max(1, args[\"n_gpu\"])\n",
        "    train_sampler = RandomSampler(train_dataset) if args[\"local_rank\"] == -1 else DistributedSampler(train_dataset) \n",
        "    #train_sampler = DistributedSampler(train_dataset)\n",
        "    #train_dataloader = DataLoader(train_dataset,  sampler=train_sampler, batch_size=args[\"train_batch_size\"])\n",
        "    train_dataloader = DataLoader(train_dataset,  shuffle=False, sampler=None, batch_size=args[\"train_batch_size\"])\n",
        "    \n",
        "    if args[\"max_steps\"] > 0:\n",
        "        t_total = args[\"max_steps\"]\n",
        "        args[\"num_train_epochs\"] = args[\"max_steps\"] // (len(train_dataloader) // args[\"gradient_accumulation_steps\"]) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args[\"gradient_accumulation_steps\"] * args[\"num_train_epochs\"]\n",
        "\n",
        "    # Prepare optimizer and scheduler (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args[\"weight_decay\"]},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, \n",
        "                      lr=args[\"learning_rate\"], \n",
        "                      eps=args[\"adam_epsilon\"])\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                     num_warmup_steps=args[\"warmup_steps\"], \n",
        "                                     num_training_steps=t_total)\n",
        "    \n",
        "    # Multiple GPU training\n",
        "    if args[\"n_gpu\"] > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training\n",
        "    if args[\"local_rank\"] != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, \n",
        "                                                          device_ids=[args[\"local_rank\"]],\n",
        "                                                          output_device=args[\"local_rank\"],\n",
        "                                                          find_unused_parameters=True)\n",
        "    \n",
        "    # Training\n",
        "    print(\"***** Running training *****\")\n",
        "    print(\"  Num examples = %d\", len(train_dataset))\n",
        "    print(\"  Num Epochs = %d\", args[\"num_train_epochs\"])\n",
        "    print(\"  Instantaneous batch size per GPU = %d\", args[\"per_gpu_train_batch_size\"])\n",
        "    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                   args[\"train_batch_size\"] * args[\"gradient_accumulation_steps\"] * (torch.distributed.get_world_size() if args[\"local_rank\"] != -1 else 1))\n",
        "    print(\"  Gradient Accumulation steps = %d\", args[\"gradient_accumulation_steps\"])\n",
        "    print(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    # Added here for reproductibility (even between python 2 and 3)\n",
        "    set_seed(args)  \n",
        "    for _ in range(int(args[\"num_train_epochs\"])):\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args[\"device\"]) for t in batch)\n",
        "            inputs = {'input_ids':       batch[0],\n",
        "                      'attention_mask':  batch[1],\n",
        "                      'token_type_ids':  None if args[\"model_type\"] in ['xlm', 'roberta'] else batch[2],\n",
        "                      'start_positions': batch[3],\n",
        "                      'end_positions':   batch[4]}\n",
        "\n",
        "            if args[\"model_type\"] in ['xlnet', 'xlm']:\n",
        "                inputs.update({'cls_index': batch[5],\n",
        "                               'p_mask': batch[6]})\n",
        "            outputs = model(**inputs)\n",
        "            # Model outputs are always tuple in transformers (see doc)\n",
        "            loss = outputs[0]  \n",
        "\n",
        "            if args[\"n_gpu\"] > 1:\n",
        "                # `mean()` to average on multi-gpu parallel (not distributed) training\n",
        "                loss = loss.mean() \n",
        "            if args[\"gradient_accumulation_steps\"] > 1:\n",
        "                loss = loss / args[\"gradient_accumulation_steps\"]\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args[\"max_grad_norm\"])\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args[\"gradient_accumulation_steps\"] == 0:\n",
        "                optimizer.step()\n",
        "                # Update learning rate schedule\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                #if args[\"local_rank\"] in [-1, 0] and args[\"logging_steps\"] > 0 and global_step % args[\"logging_steps\"] == 0:\n",
        "                    # Log metrics\n",
        "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                    #if local_rank == -1 and evaluate_during_training:  \n",
        "                    #    results = evaluate(args, model, tokenizer)\n",
        "                    #    for key, value in results.items():\n",
        "                    #        tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                    #tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    #tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args[\"logging_steps\"], global_step)\n",
        "                    #logging_loss = tr_loss\n",
        "\n",
        "                if args[\"local_rank\"] in [-1, 0] and args[\"save_steps\"] > 0 and global_step % args[\"save_steps\"] == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args[\"output_dir\"], 'checkpoint-{}'.format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    print(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "            if args[\"max_steps\"] > 0 and global_step > args[\"max_steps\"]:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "                \n",
        "        if args[\"max_steps\"] > 0 and global_step > args[\"max_steps\"]:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    #if args[\"local_rank\"] in [-1, 0]:\n",
        "    #    tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY9Hd1RvmP17"
      },
      "source": [
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True, model_type=args[\"model_type\"])\n",
        "\n",
        "    if not os.path.exists(args[\"output_dir\"]) and args[\"local_rank\"] in [-1, 0]:\n",
        "        os.makedirs(args[\"output_dir\"])\n",
        "\n",
        "    args[\"eval_batch_size\"] = args[\"per_gpu_eval_batch_size\"] * max(1, args[\"n_gpu\"])\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args[\"eval_batch_size\"])\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args[\"n_gpu\"] > 1 and not isinstance(model, torch.nn.DataParallel):\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    print(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    print(\"  Num examples = %d\", len(dataset))\n",
        "    print(\"  Batch size = %d\", args[\"eval_batch_size\"])\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in eval_dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args[\"device\"]) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                'token_type_ids': None if args[\"model_type\"] in ['xlm', 'roberta'] else batch[2]  # XLM don't use segment_ids\n",
        "            }\n",
        "\n",
        "            if args[\"model_type\"] in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            # XLNet and XLM use more arguments for their predictions\n",
        "            if args[\"model_type\"] in [\"xlnet\", \"xlm\"]:\n",
        "                inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n",
        "                # for lang_id-sensitive xlm models\n",
        "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
        "                    inputs.update(\n",
        "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args[\"lang_id\"]).to(args[\"device\"])}\n",
        "                    )\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            try:\n",
        "              eval_feature = features[example_index.item()]\n",
        "              unique_id = int(eval_feature.unique_id)\n",
        "            except: print(example_index.item(), len(features))\n",
        "\n",
        "            output = [to_list(output[i]) for output in outputs]\n",
        "\n",
        "            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n",
        "            # models only use two.\n",
        "            if len(output) >= 5:\n",
        "                start_logits = output[0]\n",
        "                start_top_index = output[1]\n",
        "                end_logits = output[2]\n",
        "                end_top_index = output[3]\n",
        "                cls_logits = output[4]\n",
        "\n",
        "                result = SquadResult(\n",
        "                    unique_id,\n",
        "                    start_logits,\n",
        "                    end_logits,\n",
        "                    start_top_index=start_top_index,\n",
        "                    end_top_index=end_top_index,\n",
        "                    cls_logits=cls_logits,\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                start_logits, end_logits = output\n",
        "                result = SquadResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    print(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args[\"output_dir\"], \"predictions_{}.json\".format(prefix))\n",
        "    output_nbest_file = os.path.join(args[\"output_dir\"], \"nbest_predictions_{}.json\".format(prefix))\n",
        "\n",
        "    if args[\"version_2_with_negative\"]:\n",
        "        output_null_log_odds_file = os.path.join(args[\"output_dir\"], \"null_odds_{}.json\".format(prefix))\n",
        "    else:\n",
        "        output_null_log_odds_file = None\n",
        "\n",
        "    # XLNet and XLM use a more complex post-processing procedure\n",
        "    if args[\"model_type\"] in [\"xlnet\", \"xlm\"]:\n",
        "        start_n_top = model.config.start_n_top if hasattr(model, \"config\") else model.module.config.start_n_top\n",
        "        end_n_top = model.config.end_n_top if hasattr(model, \"config\") else model.module.config.end_n_top\n",
        "\n",
        "        predictions = compute_predictions_log_probs(\n",
        "            examples,\n",
        "            features,\n",
        "            all_results,\n",
        "            args[\"n_best_size\"],\n",
        "            args[\"max_answer_length\"],\n",
        "            output_prediction_file,\n",
        "            output_nbest_file,\n",
        "            output_null_log_odds_file,\n",
        "            start_n_top,\n",
        "            end_n_top,\n",
        "            args[\"version_2_with_negative\"],\n",
        "            tokenizer,\n",
        "            args[\"verbose_logging\"],\n",
        "        )\n",
        "    else:\n",
        "        predictions = compute_predictions_logits(\n",
        "            examples,\n",
        "            features,\n",
        "            all_results,\n",
        "            args[\"n_best_size\"],\n",
        "            args[\"max_answer_length\"],\n",
        "            args[\"do_lower_case\"],\n",
        "            output_prediction_file,\n",
        "            output_nbest_file,\n",
        "            output_null_log_odds_file,\n",
        "            args[\"verbose_logging\"],\n",
        "            args[\"version_2_with_negative\"],\n",
        "            args[\"null_score_diff_threshold\"],\n",
        "            tokenizer, args[\"model_type\"]\n",
        "        )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "80b8d53b7e554790832c3ddedc634131",
            "39538b2d5fe7432183f9aa37f7961d33",
            "d61c225e726f458bbf9e29dfefa553b2",
            "7c8892ccc645404984675cebb07e1090",
            "d2a29f3054b34a7f8163eab1be9466b5",
            "4c8d3aa814fd4f34b69cfac7662a2e59",
            "3ed0f42cd0444b298621db6867ccda57",
            "7154144a7c3841528c42c3af1893eb11",
            "32a920242fce4b6e8c400ba5ecd14959",
            "b76420ee3572492da49f57834d6cba87",
            "0257e5a8b17143329ef5178d8a74f614",
            "ea39e4507abb4ff3a12891f101959163",
            "4f6714ca20a3476d8eedba10eb4e3640",
            "df0736c91bf04b159b1e3a08a0220c1f",
            "41aa656e1ee94c40bae8af84be1566c2",
            "9627fc88be6b475d8cab04efbf0a2b38",
            "86b12e9ff8554b27a40425a9e30f2d37",
            "84537af88e6c4bceac4f7c37e718acea",
            "8fe6dfc9291f4a70a26a1948106a3f60",
            "d99c2d11602d4534bcbfc064e5735201",
            "81241631caf645d9b0ac6280e8e52ca6",
            "0a1a18f000584773b8b8e4f2d4177c08",
            "eba7a0fdb98c4a7d9564d0acb36b4af7",
            "ec90d0a63e9c4b5d92efc26681bfcd9c"
          ]
        },
        "id": "J6OZjR1ROzr-",
        "outputId": "3cc258ec-7997-4ea5-cab3-defd76ffd2e1"
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForQuestionAnswering, BertTokenizerFast),\n",
        "    #'bert': (BertConfig, BertForQuestionAnswering, BertTokenizerFast),\n",
        "    #'roberta': (RobertaConfig, RobertaForQuestionAnswering, RobertaTokenizer)\n",
        "    'roberta': (XLMRobertaConfig, XLMRobertaForQuestionAnswering, XLMRobertaTokenizer)\n",
        "}\n",
        "\n",
        "args = {\n",
        "    \"local_rank\": -1,\n",
        "    \"model_type\": \"roberta\",\n",
        "    \"config_name\": \"\", \n",
        "    #\"model_name_or_path\": \"bert-base-multilingual-cased\", \n",
        "    \"model_name_or_path\": \"xlm-roberta-base\",\n",
        "    \"tokenizer_name\": \"\",\n",
        "    \"max_seq_length\": 400,  \n",
        "    \"overwrite_cache\": True,\n",
        "    #\"do_lower_case\": True,\n",
        "    \"do_lower_case\": False,\n",
        "    \"do_train\": True,\n",
        "    \"output_dir\": \"models/xlmr-finetuned-viquad_k5\", \n",
        "    \"version_2_with_negative\": True,\n",
        "    \"doc_stride\": 128,\n",
        "    \"max_query_length\": 128,\n",
        "    \"train_file\": \"./ViQuAD1.1/top_5/train_ViQuAD_top_5.json\",\n",
        "    \"predict_file\":\"./ViQuAD1.1/top_5/dev_ViQuAD_top_5.json\",\n",
        "    \"per_gpu_train_batch_size\": 4, \n",
        "    \"max_steps\": -1,\n",
        "    \"num_train_epochs\": 2,\n",
        "    \"learning_rate\": 2e-5,   \n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    \"warmup_steps\": 100,   \n",
        "    \"no_cuda\": False,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"weight_decay\": 0.0,  \n",
        "    \"save_steps\": 1000,\n",
        "    \"seed\": 42,\n",
        "    \"do_eval\": True,\n",
        "    \"eval_all_checkpoints\": True,\n",
        "    \"eval_batch_size\": 8,\n",
        "    \"per_gpu_eval_batch_size\": 4,  \n",
        "    \"n_best_size\": 20,\n",
        "    \"max_answer_length\": 500,\n",
        "    \"null_score_diff_threshold\": 0.0,\n",
        "    \"verbose_logging\": True,\n",
        "    \"version_2_with_negative\": True\n",
        "    \n",
        "}\n",
        "\n",
        "if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n",
        "    args[\"n_gpu\"] = torch.cuda.device_count()\n",
        "else:  \n",
        "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args[\"local_rank\"])\n",
        "    device = torch.device(\"cuda\", args[\"local_rank\"])\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args[\"n_gpu\"] = 1\n",
        "    \n",
        "args[\"device\"] = device\n",
        "    \n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n",
        "config = config_class.from_pretrained(args[\"config_name\"] if args[\"config_name\"] else args[\"model_name_or_path\"])\n",
        "tokenizer = tokenizer_class.from_pretrained(args[\"tokenizer_name\"] if args[\"tokenizer_name\"] else args[\"model_name_or_path\"], do_lower_case=args[\"do_lower_case\"], strip_accents=False, add_special_tokens=False)\n",
        "model = model_class.from_pretrained(args[\"model_name_or_path\"], from_tf=bool('.ckpt' in args[\"model_name_or_path\"]), config=config)\n",
        "\n",
        "# Make sure only the first process in distributed training will download model & vocab\n",
        "if args[\"local_rank\"] == 0:\n",
        "    torch.distributed.barrier()  \n",
        "\n",
        "model.to(args[\"device\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80b8d53b7e554790832c3ddedc634131",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32a920242fce4b6e8c400ba5ecd14959",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86b12e9ff8554b27a40425a9e30f2d37",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaForQuestionAnswering(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEiSRHdqOzsB"
      },
      "source": [
        "print(\"Training/evaluation parameters %s\", args)\n",
        "    \n",
        "# Training\n",
        "if args[\"do_train\"]:\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False, model_type=args[\"model_type\"])\n",
        "\n",
        "    global_step, tr_loss = train(args, \n",
        "                                 train_dataset, \n",
        "                                 model, \n",
        "                                 tokenizer)\n",
        "    print(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "# Save the trained model and the tokenizer\n",
        "if args[\"do_train\"] and (args[\"local_rank\"] == -1 or torch.distributed.get_rank() == 0):\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(args[\"output_dir\"]) and args[\"local_rank\"] in [-1, 0]:\n",
        "        os.makedirs(args[\"output_dir\"])\n",
        "\n",
        "    print(\"Saving model checkpoint to %s\", args[\"output_dir\"])\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    # Take care of distributed/parallel training\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  \n",
        "    model_to_save.save_pretrained(args[\"output_dir\"])\n",
        "    tokenizer.save_pretrained(args[\"output_dir\"])\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(args, os.path.join(args[\"output_dir\"], 'training_args.bin'))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = model_class.from_pretrained(args[\"output_dir\"])\n",
        "    tokenizer = tokenizer_class.from_pretrained(args[\"output_dir\"], do_lower_case=args[\"do_lower_case\"])\n",
        "    model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKowphg6OzsE"
      },
      "source": [
        "# Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
        "results = {}\n",
        "if args[\"do_eval\"] and args[\"local_rank\"] in [-1, 0]:\n",
        "    checkpoints = [args[\"output_dir\"]]\n",
        "    if args[\"eval_all_checkpoints\"]:\n",
        "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args[\"output_dir\"] + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
        "\n",
        "    print(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "    for checkpoint in checkpoints:\n",
        "        # Reload the model\n",
        "        global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "        model = model_class.from_pretrained(checkpoint)\n",
        "        model.to(device)\n",
        "\n",
        "        # Evaluate\n",
        "        result = evaluate(args, model, tokenizer, prefix=global_step)\n",
        "\n",
        "        result = dict((k + ('_{}'.format(global_step) if global_step else ''), v) for k, v in result.items())\n",
        "        results.update(result)\n",
        "\n",
        "print(\"Results: {}\".format(results))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB0IUWI1M9x3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffb1f74-fb9e-4e4a-c75e-6ffb177a3452"
      },
      "source": [
        "print(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'exact_1000': 44.73222124670764, 'f1_1000': 65.29883232242969, 'total_1000': 2278, 'HasAns_exact_1000': 44.73222124670764, 'HasAns_f1_1000': 65.29883232242969, 'HasAns_total_1000': 2278, 'best_exact_1000': 44.73222124670764, 'best_exact_thresh_1000': 0.0, 'best_f1_1000': 65.29883232242969, 'best_f1_thresh_1000': 0.0, 'exact_10000': 62.59877085162423, 'f1_10000': 80.57737154454257, 'total_10000': 2278, 'HasAns_exact_10000': 62.59877085162423, 'HasAns_f1_10000': 80.57737154454257, 'HasAns_total_10000': 2278, 'best_exact_10000': 62.59877085162423, 'best_exact_thresh_10000': 0.0, 'best_f1_10000': 80.57737154454257, 'best_f1_thresh_10000': 0.0, 'exact_11000': 62.159789288849865, 'f1_11000': 79.63299200142305, 'total_11000': 2278, 'HasAns_exact_11000': 62.159789288849865, 'HasAns_f1_11000': 79.63299200142305, 'HasAns_total_11000': 2278, 'best_exact_11000': 62.159789288849865, 'best_exact_thresh_11000': 0.0, 'best_f1_11000': 79.63299200142305, 'best_f1_thresh_11000': 0.0, 'exact_12000': 64.39859525899912, 'f1_12000': 81.9434075717986, 'total_12000': 2278, 'HasAns_exact_12000': 64.39859525899912, 'HasAns_f1_12000': 81.9434075717986, 'HasAns_total_12000': 2278, 'best_exact_12000': 64.39859525899912, 'best_exact_thresh_12000': 0.0, 'best_f1_12000': 81.9434075717986, 'best_f1_thresh_12000': 0.0, 'exact_13000': 64.09130816505707, 'f1_13000': 82.13922500827759, 'total_13000': 2278, 'HasAns_exact_13000': 64.09130816505707, 'HasAns_f1_13000': 82.13922500827759, 'HasAns_total_13000': 2278, 'best_exact_13000': 64.09130816505707, 'best_exact_thresh_13000': 0.0, 'best_f1_13000': 82.13922500827759, 'best_f1_thresh_13000': 0.0, 'exact_14000': 66.11062335381914, 'f1_14000': 83.0787555135739, 'total_14000': 2278, 'HasAns_exact_14000': 66.11062335381914, 'HasAns_f1_14000': 83.0787555135739, 'HasAns_total_14000': 2278, 'best_exact_14000': 66.11062335381914, 'best_exact_thresh_14000': 0.0, 'best_f1_14000': 83.0787555135739, 'best_f1_thresh_14000': 0.0, 'exact_2000': 35.99648814749781, 'f1_2000': 66.15188521909019, 'total_2000': 2278, 'HasAns_exact_2000': 35.99648814749781, 'HasAns_f1_2000': 66.15188521909019, 'HasAns_total_2000': 2278, 'best_exact_2000': 35.99648814749781, 'best_exact_thresh_2000': 0.0, 'best_f1_2000': 66.15188521909019, 'best_f1_thresh_2000': 0.0, 'exact_3000': 50.263388937664615, 'f1_3000': 72.72038202421241, 'total_3000': 2278, 'HasAns_exact_3000': 50.263388937664615, 'HasAns_f1_3000': 72.72038202421241, 'HasAns_total_3000': 2278, 'best_exact_3000': 50.263388937664615, 'best_exact_thresh_3000': 0.0, 'best_f1_3000': 72.72038202421241, 'best_f1_thresh_3000': 0.0, 'exact_4000': 58.208955223880594, 'f1_4000': 77.45065970421184, 'total_4000': 2278, 'HasAns_exact_4000': 58.208955223880594, 'HasAns_f1_4000': 77.45065970421184, 'HasAns_total_4000': 2278, 'best_exact_4000': 58.208955223880594, 'best_exact_thresh_4000': 0.0, 'best_f1_4000': 77.45065970421184, 'best_f1_thresh_4000': 0.0, 'exact_5000': 60.35996488147498, 'f1_5000': 78.89826094607358, 'total_5000': 2278, 'HasAns_exact_5000': 60.35996488147498, 'HasAns_f1_5000': 78.89826094607358, 'HasAns_total_5000': 2278, 'best_exact_5000': 60.35996488147498, 'best_exact_thresh_5000': 0.0, 'best_f1_5000': 78.89826094607358, 'best_f1_thresh_5000': 0.0, 'exact_6000': 60.316066725197544, 'f1_6000': 78.1661686936534, 'total_6000': 2278, 'HasAns_exact_6000': 60.316066725197544, 'HasAns_f1_6000': 78.1661686936534, 'HasAns_total_6000': 2278, 'best_exact_6000': 60.316066725197544, 'best_exact_thresh_6000': 0.0, 'best_f1_6000': 78.1661686936534, 'best_f1_thresh_6000': 0.0, 'exact_7000': 56.80421422300263, 'f1_7000': 75.9595902667912, 'total_7000': 2278, 'HasAns_exact_7000': 56.80421422300263, 'HasAns_f1_7000': 75.9595902667912, 'HasAns_total_7000': 2278, 'best_exact_7000': 56.80421422300263, 'best_exact_thresh_7000': 0.0, 'best_f1_7000': 75.9595902667912, 'best_f1_thresh_7000': 0.0, 'exact_8000': 62.42317822651449, 'f1_8000': 79.57430795804369, 'total_8000': 2278, 'HasAns_exact_8000': 62.42317822651449, 'HasAns_f1_8000': 79.57430795804369, 'HasAns_total_8000': 2278, 'best_exact_8000': 62.42317822651449, 'best_exact_thresh_8000': 0.0, 'best_f1_8000': 79.57430795804369, 'best_f1_thresh_8000': 0.0, 'exact_9000': 61.54521510096576, 'f1_9000': 80.40675008389206, 'total_9000': 2278, 'HasAns_exact_9000': 61.54521510096576, 'HasAns_f1_9000': 80.40675008389206, 'HasAns_total_9000': 2278, 'best_exact_9000': 61.54521510096576, 'best_exact_thresh_9000': 0.0, 'best_f1_9000': 80.40675008389206, 'best_f1_thresh_9000': 0.0, 'exact_viquad_k8': 65.97892888498683, 'f1_viquad_k8': 83.0793396680307, 'total_viquad_k8': 2278, 'HasAns_exact_viquad_k8': 65.97892888498683, 'HasAns_f1_viquad_k8': 83.0793396680307, 'HasAns_total_viquad_k8': 2278, 'best_exact_viquad_k8': 65.97892888498683, 'best_exact_thresh_viquad_k8': 0.0, 'best_f1_viquad_k8': 83.0793396680307, 'best_f1_thresh_viquad_k8': 0.0}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}